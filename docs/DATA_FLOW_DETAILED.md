# üîÑ DATA FLOW ‚Äì D·ª± √Ån AMT

> T√†i li·ªáu n√†y m√¥ t·∫£ **lu·ªìng d·ªØ li·ªáu** chi ti·∫øt nh·∫•t t·ª´ d·ªØ li·ªáu th√¥ (MIDI & m√¥ t·∫£) t·ªõi ƒë·∫ßu ra nh·∫°c sinh v√† ƒë√°nh gi√°.
>
> C√°c b∆∞·ªõc ghi r√µ: **ƒë·∫ßu v√†o**, **script/h√†m th·ª±c thi**, **ƒë·∫ßu ra** (t√™n file + ƒë∆∞·ªùng d·∫´n).

---

## 1. S∆° ƒë·ªì t·ªïng qu√°t (Mermaid)
```mermaid
graph TD
    %% Ngu·ªìn d·ªØ li·ªáu
    A[Lakh MIDI Clean (\ndata/midi)] --> B1[midi_metadata.py]
    W[Wikipedia API] --> B2[wikipedia_collector.py]

    %% Trung gian
    B1 --> C1[midi_metadata_list.json]
    B2 --> C2[automated_paired_data.json]

    %% Embed & clustering
    C2 --> D1[get_bert_embeddings]
    D1 --> E1[text_embeddings.json]
    E1 --> F1[cluster_embeddings]
    F1 --> G1[clustered_text_data.json]

    %% Gh√©p v·ªõi MIDI events
    G1 & A --> H1[midi_to_event_sequence]
    H1 --> I1[data_preparation.py]
    I1 --> J1[amt_training_data.json]

    %% Hu·∫•n luy·ªán
    J1 --> K1[training.py]
    K1 --> L1[GPT-2 checkpoint]

    %% Sinh & ƒë√°nh gi√°
    L1 --> M1[AMTGenerator.generate_music]
    M1 --> N1[generated.mid]
    A --> Ref[reference.mid]
    N1 & Ref --> O1[evaluation.metrics]
    O1 --> P1[eval_report.json]
```

---

## 2. B·∫£ng tu·∫ßn t·ª± (Input ‚Üí Output)
| # | B∆∞·ªõc | Script/H√†m ch√≠nh | ƒê·∫ßu v√†o | ƒê·∫ßu ra |
|---|------|------------------|---------|--------|
| 1 | Li·ªát k√™ MIDI | `data_collection/midi_metadata.py` <br> `list_midi_files_and_metadata()` | `data/midi/**/*.mid` | `data/output/midi_metadata_list.json` |
| 2 | Gh√©p Wikipedia | `data_collection/wikipedia_collector.py` <br> `pair_midi_with_wikipedia()` | File JSON b∆∞·ªõc 1 | `data/output/automated_paired_data.json` (th√™m `text_description`) |
| 3 | Text Embedding | `data_processing/text_processor.py` <br> `get_bert_embeddings()` | JSON b∆∞·ªõc 2 | `data/output/text_embeddings.json` (th√™m `embedding`) |
| 4 | Clustering | `model/clustering.py` <br> `cluster_embeddings()` | JSON b∆∞·ªõc 3 | `data/output/clustered_text_data.json` (th√™m `semantic_token`) |
| 5 | Chu·ªói s·ª± ki·ªán + Token | `utils/data_preparation.py` <br> `midi_to_event_sequence()` | MIDI g·ªëc + JSON b∆∞·ªõc 4 | `data/output/amt_training_data.json` (ch·ª©a `combined_sequence_for_amt`) |
| 6 | Hu·∫•n luy·ªán | `model/training.py` <br> `train_model()` | Training JSON | `models/checkpoints/checkpoint_epoch_*.pt` |
| 7 | Sinh nh·∫°c | `model/generation.py` <br> `AMTGenerator.generate_music()` | Checkpoint + prompt text | `output/generated/*.mid` |
| 8 | ƒê√°nh gi√° | `evaluation/metrics.py` <br> `evaluate_generated_music()` | MIDI g·ªëc tham chi·∫øu + MIDI sinh | `data/evaluation/eval_report.json` |

---

## 3. Chi ti·∫øt t·ª´ng kh·ªëi
### 3.1 Li·ªát k√™ & ghi metadata
- **Function:** `list_midi_files_and_metadata(base_dir)`  
- **Logic:** duy·ªát ƒë·ªá quy, b·∫Øt t√™n ngh·ªá sƒ© t·ª´ `os.path.basename(root)`, r·ª≠a t√™n b√†i h√°t (`_` ‚Üí space, xo√° s·ªë ƒëu√¥i).  
- **T·∫ßn su·∫•t ch·∫°y:** m·ªôt l·∫ßn chu·∫©n b·ªã d·ªØ li·ªáu.

### 3.2 Gh√©p m√¥ t·∫£ Wikipedia
- **Thu·∫≠t to√°n t√¨m trang:** th·ª≠ chu·ªói t√¨m ki·∫øm theo th·ª© t·ª±: "`artist title (song)`", "‚Ä¶".  
- D√πng th∆∞ vi·ªán `wikipedia`; b·∫Øt l·ªói Page/Disambiguation.

### 3.3 T√≠nh embedding BERT
- **Model:** `bert-base-uncased` (Transformers).  
- **Truncate:** `max_length=512` token, l·∫•y embedding token `[CLS]`.

### 3.4 Clustering semantic token
- **Embedding matrix:** N √ó 768.  
- **K determination:** silhouette score (min_k=2, max_k ‚â§ 10).  
- **KMeans:** `sklearn.cluster.KMeans` with `n_init="auto"`.

### 3.5 Gh√©p token & s·ª± ki·ªán MIDI
- **Tr√≠ch event:** (TIME_ON, NOTE, DUR) v·ªõi l∆∞·ª£ng t·ª≠ 480 ticks.  
- **Semantic token:** format string `SEMANTIC_TOKEN_i` ƒë·ªám ƒë·∫ßu chu·ªói.  
- **Output:** list JSON item ch·ª©a c·∫£ vector embedding (ƒë·ªÉ hu·∫•n luy·ªán ƒëa ph∆∞∆°ng th·ª©c).

### 3.6 Hu·∫•n luy·ªán m√¥ h√¨nh
- **Dataset:** m·ªói sample ‚Üí tuple `(text_embedding, event_sequence)`.
- **Model:** projection + GPT-2 (6L, 8H, 1 024H, vocab 512).  
- **Loss:** t·ª± ƒë·ªông qua `GPT2LMHeadModel` (`outputs.loss`).

### 3.7 Sinh nh·∫°c
- **ƒêi·ªÅu khi·ªÉn:** `temperature`, `top_k`, `top_p`.  
- **Gi·∫£i m√£ token ID ‚Üí triplet** (h√†m TODO n·∫øu ch∆∞a ho√†n th√†nh).

### 3.8 ƒê√°nh gi√°
- **Metric:** density, velocity, range, time-sig, tempo.  
- **Output:** `overall_score` ‚àà [0,1].

---

## 4. File & th∆∞ m·ª•c ƒë∆∞·ª£c t·∫°o theo th·ªùi gian
```text
[data/midi]             (input c·ªë ƒë·ªãnh)
‚îî‚îÄ Step1 -> data/output/midi_metadata_list.json
‚îî‚îÄ Step2 -> data/output/automated_paired_data.json
‚îî‚îÄ Step3 -> data/output/text_embeddings.json
‚îî‚îÄ Step4 -> data/output/clustered_text_data.json
‚îî‚îÄ Step5 -> data/output/amt_training_data.json
‚îî‚îÄ Step6 -> models/checkpoints/checkpoint_epoch_*.pt
‚îî‚îÄ Step7 -> output/generated/*.mid
‚îî‚îÄ Step8 -> data/evaluation/eval_report.json
```

---

## 5. Tr√¨nh t·ª± th·ª±c thi g·ª£i √Ω (CLI)
```bash
cd AMT
# 1‚Äì5 trong m·ªôt l·ªánh
python run.py pipeline

# 6 Hu·∫•n luy·ªán
python run.py train

# 7 Sinh nh·∫°c
python - <<'PY'
from AMT.source.model.generation import AMTGenerator
model = AMTGenerator("models/checkpoints/checkpoint_epoch_10.pt")
model.generate_music("A calm piano piece", "output/generated/calm_piano.mid")
PY

# 8 ƒê√°nh gi√°
python - <<'PY'
from AMT.source.evaluation.metrics import evaluate_generated_music
print(evaluate_generated_music("data/reference/ref.mid", "output/generated/calm_piano.mid"))
PY
```

---
**T√†i li·ªáu ho√†n th√†nh ‚Äì Lu·ªìng d·ªØ li·ªáu ƒë∆∞·ª£c m√¥ t·∫£ ƒë·∫ßy ƒë·ªß cho vi·ªác t√°i l·∫≠p v√† debug.** 